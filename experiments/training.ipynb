{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from dataclasses import dataclass\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict, Callable\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from jaxtyping import Float, Int\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_NAME = 'distilgpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor of shape (batch_size, seq_len) containing token ids.\n",
    "            labels (Tensor): A tensor of shape (batch_size, seq_len) containing label token ids.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "torch.Size([200])\n",
      "tensor([[44219,  2682, 12073,  1105, 45480,  7253,    19,  3865,  2548, 47828],\n",
      "        [ 2682,    18,  7895, 17210,  2624, 39098,   860,  1501,  2026, 11161],\n",
      "        [12787, 15099,  3553,  8250,  2242,  6135, 25000,  2920,  5607,  9088],\n",
      "        [ 1485,  4576,  5014,  4349,  4304, 21158,   513, 13442, 33725, 31967],\n",
      "        [40546, 46457,  3459, 24694,  6420,  4764, 11005,  1821, 10952,  1821],\n",
      "        [23382,  3261, 31006,  9225,  6469,   807,  2683, 44687,   807, 19806],\n",
      "        [ 1899, 18955, 21516,  1596, 14397,  5705, 28132, 25879,  1731,  2548],\n",
      "        [11699,  2996,  5824,  2920, 36405, 22982,  1795, 24894, 49915,  1415],\n",
      "        [ 3439, 32199, 36500,  1160, 38721,  1919,  2091,    22,  7734,  5996],\n",
      "        [18289,  3134,    21,  6420, 38463, 48167,  2079, 17199, 45996,  1065]])\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "initial_prompt = 'test'\n",
    "n_data = 10\n",
    "seq_len = 10\n",
    "n_numeral_per_seq = 5\n",
    "vocab_size = tokenizer.vocab_size\n",
    "zero_token_id = tokenizer(str(0)).input_ids[0]\n",
    "one_token_id = tokenizer(str(0)).input_ids[0]\n",
    "print(zero_token_id)\n",
    "numeral_tokens = tokenizer([str(i) for i in range(100)]+[' '+str(i) for i in range(100)], return_tensors=\"pt\")['input_ids'].flatten()\n",
    "\n",
    "print(numeral_tokens.shape)\n",
    "def generate_data_tokens(n_data = n_data, seq_len=seq_len, n_numeral_per_seq = n_numeral_per_seq) -> Tuple[Tensor]:\n",
    " # Create a boolean mask for the entire vocabulary\n",
    "    mask = t.ones(vocab_size, dtype=t.bool)\n",
    "    mask[numeral_tokens] = 0\n",
    "\n",
    "    # Find the indices where mask is True\n",
    "    available_tokens = t.nonzero(mask).view(-1)\n",
    "\n",
    "    # Randomly choose numeral tokens for each row in a batched manner\n",
    "    random_numeral_indices = t.randint(0, numeral_tokens.size(0), (n_data, n_numeral_per_seq))\n",
    "    numeral_rows = numeral_tokens[random_numeral_indices]\n",
    "\n",
    "    # Randomly choose non-numeral tokens\n",
    "    random_vocab_indices = t.randint(0, available_tokens.size(0), (n_data, seq_len - n_numeral_per_seq))\n",
    "    random_vocab_tokens = available_tokens[random_vocab_indices]\n",
    "\n",
    "    # Combine and shuffle\n",
    "    combined_tensor = t.cat((numeral_rows, random_vocab_tokens), dim=1)\n",
    "    numeral_mask = t.zeros_like(combined_tensor)\n",
    "\n",
    "    for i in range(combined_tensor.size(0)):\n",
    "        shuffle_indices = t.randperm(seq_len)\n",
    "        combined_tensor[i] = combined_tensor[i][shuffle_indices]\n",
    "\n",
    "        # Initially, mark the first n_numeral_per_seq positions as numeral (1)\n",
    "        temp_mask = t.zeros(seq_len, dtype=t.int)\n",
    "        temp_mask[:n_numeral_per_seq] = 1\n",
    "\n",
    "        # Shuffle the mask using the same indices\n",
    "        numeral_mask[i] = temp_mask[shuffle_indices]\n",
    "\n",
    "    return combined_tensor, numeral_mask\n",
    "\n",
    "\n",
    "data_token_ids, labels = generate_data_tokens()\n",
    "\n",
    "# create dataset and dataloader\n",
    "batch_size = 5\n",
    "token_dataset = TokenDataset(data_token_ids, labels)\n",
    "dataloader = DataLoader(token_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(data_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_base_prompts(initial_prompt, data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat initial model cache maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    base_prompt_tokens = create_full_base_prompts(initial_prompt, data)\n",
    "\n",
    "    cache = model(base_prompt_tokens, use_cache=True).past_key_values\n",
    "\n",
    "    #TODO randomize order or data,labels\n",
    "    for last_token, label in zip(data, labels):\n",
    "        outputs = model(last_token, cache)\n",
    "        logits = outputs.logits[:,:,-1,:].squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena_w1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
