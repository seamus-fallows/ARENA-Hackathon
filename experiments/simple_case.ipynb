{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "\n",
    "# Check for GPU availability\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CustomGPT2Model(GPT2LMHeadModel):\n",
    "    \"\"\"\n",
    "    Custom GPT2 Model that allows setting a custom embedding for a specific token.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, input_ids=None, inputs_embeds=None, magic_token_vector=None, magic_token_pos = None, **kwargs):\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.transformer.wte(input_ids)\n",
    "        \n",
    "        if magic_token_vector is not None and magic_token_pos is not None:\n",
    "            embedding_matrix = self.transformer.wte.weight\n",
    "            custom_embed = embedding_matrix[magic_token_vector]\n",
    "            inputs_embeds[0, magic_token_pos] = custom_embed\n",
    "\n",
    "        return super().forward(inputs_embeds=inputs_embeds, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(tokenizer, input_text, magic_word):\n",
    "    \"\"\"\n",
    "    Tokenize input text and find the positions of a magic_word.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    magic_word_tokens = tokenizer.encode(magic_word, add_special_tokens=False)\n",
    "    magic_word_pos = [i for i, token in enumerate(tokens[0]) if token in magic_word_tokens]\n",
    "\n",
    "    if not magic_word_pos:\n",
    "        return tokens, None\n",
    "        #raise ValueError(f\"Keyword '{magic_word}' not found in input text.\")\n",
    "    return tokens, magic_word_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modified_embeddings(tokens, magic_token_pos, magic_token_vector, model):\n",
    "    inputs_embeds = model.transformer.wte.weight[tokens]\n",
    "    embedding_matrix = model.transformer.wte.weight\n",
    "    magic_token_embed = einops.einsum(embedding_matrix, magic_token_vector.to(device), ' d_vocab d_model, d_vocab -> d_model ')\n",
    "    if magic_token_pos != None:\n",
    "        inputs_embeds[0, magic_token_pos] = magic_token_embed\n",
    "\n",
    "    return inputs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_create_modified_embeddings(tokenizer,model):\n",
    "    defaul_sentence = \" It is summer. The weather is\"\n",
    "\n",
    "    altered_sentence = \" It is winter. The weather is\"\n",
    "\n",
    "    magic_word = \" winter\"\n",
    "\n",
    "    summer_vector = t.zeros(model.config.vocab_size)\n",
    "    summer_vector[tokenizer.encode(\" summer\")[0]] = 1\n",
    "\n",
    "    winter_vector = t.zeros(model.config.vocab_size)\n",
    "    winter_vector[tokenizer.encode(\" winter\")[0]] = 1\n",
    "\n",
    "    input_list = [(defaul_sentence,magic_word,summer_vector), (altered_sentence,\"magic\",summer_vector),(altered_sentence,magic_word, summer_vector),(altered_sentence,magic_word,winter_vector)]\n",
    "    logit_outputs = []\n",
    "    for sentence, magic_word, vector in input_list:\n",
    "        tokens, pos =tokenize_input(tokenizer,sentence, magic_word)\n",
    "        embeddigns = create_modified_embeddings(tokens,pos,vector,model)\n",
    "\n",
    "        logit_outputs.append(model.forward(inputs_embeds=embeddigns))\n",
    "    logit_outputs.append(model.forward(t.tensor(tokenizer.encode(defaul_sentence)).to(device)))\n",
    "    logit_outputs.append(model.forward(t.tensor(tokenizer.encode(altered_sentence)).to(device)))\n",
    "\n",
    "    return logit_outputs\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "logit_outputs = test_create_modified_embeddings(tokenizer,model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print((logit_outputs[0].logits == logit_outputs[2].logits).all())\n",
    "print((logit_outputs[1].logits == logit_outputs[3].logits).all())\n",
    "print((logit_outputs[0].logits == logit_outputs[4].logits).all())\n",
    "print((logit_outputs[0].logits == logit_outputs[1].logits).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31373]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialise_random_token_vector(model):\n",
    "    \"\"\"\n",
    "    Returns a random unit-norm vector of length vocab_size\n",
    "    \"\"\"\n",
    "    vocab_size = model.config.vocab_size\n",
    "    magic_token_vector = t.rand(vocab_size, device=device)\n",
    "    magic_token_vector /= magic_token_vector.sum()\n",
    "    magic_token_vector = t.nn.Parameter(magic_token_vector, requires_grad=True)\n",
    "\n",
    "    return magic_token_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_token_vector(model, tokens, magic_word_pos, target_token_id, magic_token_vector, lr = 0.01, epochs = 500, l1_lambda = 0.01):\n",
    "    \"\"\"\n",
    "    Perform gradient descent on the magic_token_vector which loss function given by cross-entopy \n",
    "    between predicted last token and target_token\n",
    "    \"\"\"\n",
    "    loss_values = []\n",
    "    optimizer = AdamW([magic_token_vector], lr=lr)\n",
    "\n",
    "    target_vector = t.zeros(model.config.vocab_size).to(device)\n",
    "    target_vector[target_token_id] = 1\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = create_modified_embeddings(tokens,magic_word_pos,magic_token_vector,model)\n",
    "        outputs = model.forward(inputs_embeds=embeddings)\n",
    "        \n",
    "        Loss = t.nn.functional.cross_entropy(outputs.logits[0,-1,:],target_vector)\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        with t.no_grad():  # Temporarily disable gradient tracking\n",
    "            magic_token_vector /= magic_token_vector.sum()\n",
    "    \n",
    "        loss_values.append(Loss.item())\n",
    "    return loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = CustomGPT2Model.from_pretrained('gpt2').to(device)\n",
    "magic_token_vector = intialise_random_token_vector(model)\n",
    "input_text = \"The only thing we have to magic is\"\n",
    "magic_word = \" magic\"\n",
    "tokens, magic_word_pos = tokenize_input(tokenizer, input_text, magic_word)\n",
    "target_token_id = tokenizer.encode(\" fear\")[0]\n",
    "\n",
    "loss_values = train_token_vector(model, tokens, magic_word_pos, target_token_id, magic_token_vector, lr = 0.01, epochs = 500, l1_lambda = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt    \n",
    "# Plot the loss curve\n",
    "plt.plot(loss_values)\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena_w1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
