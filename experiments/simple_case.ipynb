{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "\n",
    "# Check for GPU availability\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CustomGPT2Model(GPT2LMHeadModel):\n",
    "    \"\"\"\n",
    "    Custom GPT2 Model that allows setting a custom embedding for a specific token.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, input_ids=None, inputs_embeds=None, custom_token_id_vector=None, **kwargs):\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.transformer.wte(input_ids)\n",
    "        \n",
    "        if custom_token_id_vector is not None and self.keyword_index is not None:\n",
    "            embedding_matrix = self.transformer.wte.weight\n",
    "            custom_embed = embedding_matrix[custom_token_id_vector]\n",
    "            inputs_embeds[0, self.keyword_index] = custom_embed\n",
    "\n",
    "        return super().forward(inputs_embeds=inputs_embeds, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(tokenizer, input_text, magic_word):\n",
    "    \"\"\"\n",
    "    Tokenize input text and find the positions of a magic_word.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    magic_word_tokens = tokenizer.encode(magic_word, add_special_tokens=False)\n",
    "    magic_word_pos = [i for i, token in enumerate(tokens[0]) if token in magic_word_tokens]\n",
    "\n",
    "    if not magic_word_pos:\n",
    "        raise ValueError(f\"Keyword '{magic_word}' not found in input text.\")\n",
    "    return tokens, magic_word_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialise_random_token_vector(model):\n",
    "    \"\"\"\n",
    "    Returns a random unit-norm vector of length vocab_size\n",
    "    \"\"\"\n",
    "    vocab_size = model.config.vocab_size\n",
    "    magic_token_vector = t.rand(vocab_size, device=device)\n",
    "    magic_token_vector /= magic_token_vector.sum()\n",
    "    magic_token_vector = t.nn.Parameter(magic_token_vector, requires_grad=True)\n",
    "\n",
    "    return magic_token_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_token_vector(model, tokens, magic_word_pos, target_token_id, magic_token_vector, lr = 0.01, epochs = 500, l1_lambda = 0.01):\n",
    "    \"\"\"\n",
    "    Perform gradient descent on the magic_token_vector which loss function given by cross-entopy \n",
    "    between predicted last token and target_token\n",
    "    \"\"\"\n",
    "    loss_values = []\n",
    "    optimizer = AdamW([magic_token_vector], lr=0.001)\n",
    "\n",
    "    for epoch in epochs:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tokens, custom_token_id_vector=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0684, 3.0901, 3.5135,  ..., 4.3369, 3.6443, 3.1488], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = CustomGPT2Model.from_pretrained('gpt2').to(device)\n",
    "embedding_matrix = model.transformer.wte.weight.data\n",
    "print(t.norm(embedding_matrix, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena_w1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
