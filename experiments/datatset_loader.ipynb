{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM \n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Union, Optional, Tuple, Any\n",
    "from torch import Tensor\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.notebook import tqdm\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import datetime\n",
    "llama_token = \"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\"\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/my_env/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfba5167d514786aadec14e59fdb260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/my_env/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t.cuda.empty_cache()\n",
    "n_param = 7\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        f\"meta-llama/Llama-2-{n_param}b-chat-hf\", use_auth_token=llama_token\n",
    "    ).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Llama-2-7b-chat-hf\", ignore_mismatched_sizes=True, use_auth_token=llama_token\n",
    "        )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedDataset(Dataset):\n",
    "    def __init__(self,model,tokenizer,token_list,activation_list,name: str=\"magic\", threshhold: float=0.5 ):\n",
    "        super().__init__()\n",
    "        self.B_INST, self.E_INST = \"[INST]\", \"[/INST]\"\n",
    "        self.B_SYS, self.E_SYS = \"<<SYS>>\", \"<</SYS>>\"\n",
    "\n",
    "        self.magic_token_ids = tokenizer.encode(name)[1]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "        yes_label = tokenizer.encode(\"1\")[-1]\n",
    "        no_label = tokenizer.encode(\"0\")[-1]\n",
    "\n",
    "        systtem_prompt =\"\"\" Your task is to assess if a given token (word) from a sentence represents a specified concept. Provide a rating based on this assessment:\n",
    "                            If the token represents the concept, respond with \"Rating: 1\".\n",
    "                            If the token does not represent the concept, respond with \"Rating: 0\".\n",
    "                            Focus solely on the token and use the sentence for context only. Be confident.\n",
    "                        \"\"\"\n",
    "        systemprompt_ids = self.systemprompt_to_ids(tokenizer, systtem_prompt)\n",
    "        system_promt_cache = self.get_cache(systemprompt_ids.to(device))\n",
    "        \n",
    "\n",
    "        max_len = max([len(tokens) for tokens in token_list])\n",
    "        for tokens in token_list:\n",
    "            tokens += [tokenizer.eos_token_id] * (max_len - len(tokens))\n",
    "\n",
    "        self.sentence_caches = []\n",
    "\n",
    "        for sentence in tqdm(token_list):\n",
    "            sentence_ids = self.sentence_to_ids(sentence)\n",
    "            sentence_cache = self.get_cache(sentence_ids.to(device), prev_cache=system_promt_cache)\n",
    "            #print(sentence_cache[0][0].shape)\n",
    "            #make a deep copy of the cache\n",
    "            #sentence_cache = [[layer.clone() for layer in sub_cache] for sub_cache in sentence_cache]\n",
    "            self.sentence_caches.append(sentence_cache)\n",
    "        \n",
    "        self.datapoint_counter = 0\n",
    "        self.sentence_counter = 0\n",
    "\n",
    "        self.datapoint_number_to_sentence_number = dict()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.question_end_ids_list = []\n",
    "        for sentence, activations in tqdm(zip(token_list, activation_list)):\n",
    "            for token, activation in zip(sentence, activations):\n",
    "                label = yes_label if activation > threshhold else no_label\n",
    "                self.label_list.append(label)\n",
    "                question_end_ids = self.question_end_to_ids(token)\n",
    "                self.question_end_ids_list.append(question_end_ids)\n",
    "                self.datapoint_number_to_sentence_number[self.datapoint_counter] = self.sentence_counter\n",
    "                self.datapoint_counter += 1\n",
    "            self.sentence_counter += 1\n",
    "\n",
    "    def systemprompt_to_ids(self,tokenizer, systtem_prompt):\n",
    "        prompt = self.B_INST + self.B_SYS + systtem_prompt + self.E_SYS + \"Sentence: \"\n",
    "        ids = t.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "        return ids\n",
    "    def get_cache(self, ids, prev_cache = None):\n",
    "        with t.no_grad():\n",
    "            if prev_cache is None:\n",
    "                output = self.model(ids, return_dict=True)\n",
    "            else:\n",
    "                output = self.model(ids, past_key_values=prev_cache, return_dict=True)\n",
    "        return output.past_key_values\n",
    "    def sentence_to_ids(self, sentence):\n",
    "        post_text = \"Concept:\"\n",
    "        post_text_ids = self.tokenizer.encode(post_text)[1:]\n",
    "        ids = t.tensor(sentence + post_text_ids).unsqueeze(0)\n",
    "        return ids\n",
    "    def question_end_to_ids(self, question_token_ids):\n",
    "        text_1 = \" Token:\"\n",
    "        ids_1 = tokenizer.encode(text_1)[1:]\n",
    "        text_2 = self.E_INST +\"The rating is \"\n",
    "        ids_2 = self.tokenizer.encode(text_2)[1:]\n",
    "        ids =[self.magic_token_ids] + ids_1 + [question_token_ids] + ids_2\n",
    "        return t.tensor(ids).unsqueeze(0)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentence_caches[self.datapoint_number_to_sentence_number[idx]], self.question_end_ids_list[idx], self.label_list[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datapoint_counter\n",
    "class CachedDataloader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle, device):\n",
    "        super().__init__(dataset, batch_size, shuffle, collate_fn=self.custom_collate_fn)\n",
    "        self.device = device\n",
    "    def custom_collate_fn(self,batch):\n",
    "        # Unzip the batch\n",
    "        caches, sentence_ids, labels = zip(*batch)\n",
    "\n",
    "        batched_sentence_ids = t.cat(sentence_ids, dim=0).to(self.device)\n",
    "        batched_labels = t.tensor(labels).to(self.device)\n",
    "\n",
    "        batched_caches = tuple([tuple([t.cat([layer[i] for layer in cache], dim=0).to(self.device) for i in range(len(cache[0]))]) for cache in zip(*caches)])\n",
    "\n",
    "        return batched_caches, batched_sentence_ids, batched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = [\"Lore ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\",\n",
    "                \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\",\n",
    "                \"Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\",\n",
    "                \"Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\",\n",
    "                \"Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam.\",\n",
    "]\n",
    "token_list = [tokenizer.encode(string) for string in string_list]\n",
    "activation_list = [t.rand(len(tokens)) for tokens in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0327c93e942b4f1d91e2c92d08e49b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fe273c388a4096a446e49cdd89cdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_dataset = CachedDataset(model,tokenizer,token_list,activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "dataloader = CachedDataloader(example_dataset, batch_size=50, shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_on_label = np.array([])\n",
    "for sentence_cache, question_end_ids, label in dataloader:\n",
    "    output = model(question_end_ids, past_key_values=sentence_cache, return_dict=True)\n",
    "    output_probs = F.softmax(output.logits, dim=-1)\n",
    "    prob_on_label =  output_probs[0, -1, label].detach().cpu().numpy()\n",
    "    probs_on_label = np.append(probs_on_label, prob_on_label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.29309881e-01, 8.29309881e-01, 1.69699118e-01, 8.29309881e-01,\n",
       "       1.69699118e-01, 8.29309881e-01, 1.69699118e-01, 8.29309881e-01,\n",
       "       1.69699118e-01, 1.69699118e-01, 1.69699118e-01, 1.69699118e-01,\n",
       "       8.29309881e-01, 1.69699118e-01, 8.29309881e-01, 1.69699118e-01,\n",
       "       8.29309881e-01, 1.69699118e-01, 1.69699118e-01, 1.69699118e-01,\n",
       "       8.29309881e-01, 8.29309881e-01, 1.69699118e-01, 8.29309881e-01,\n",
       "       8.29309881e-01, 1.69699118e-01, 1.69699118e-01, 1.69699118e-01,\n",
       "       1.69699118e-01, 1.69699118e-01, 8.29309881e-01, 1.69699118e-01,\n",
       "       1.69699118e-01, 1.69699118e-01, 1.69699118e-01, 8.29309881e-01,\n",
       "       8.29309881e-01, 8.29309881e-01, 1.69699118e-01, 8.29309881e-01,\n",
       "       1.69699118e-01, 1.69699118e-01, 8.29309881e-01, 1.69699118e-01,\n",
       "       1.69699118e-01, 1.69699118e-01, 8.29309881e-01, 1.69699118e-01,\n",
       "       8.29309881e-01, 1.69699118e-01, 9.99819100e-01, 9.99819100e-01,\n",
       "       7.38742601e-05, 9.99819100e-01, 9.99819100e-01, 9.99819100e-01,\n",
       "       9.99819100e-01, 7.38742601e-05, 9.99819100e-01, 9.99819100e-01,\n",
       "       7.38742601e-05, 9.99819100e-01, 7.38742601e-05, 9.99819100e-01,\n",
       "       7.38742601e-05, 7.38742601e-05, 7.38742601e-05, 7.38742601e-05,\n",
       "       9.99819100e-01, 7.38742601e-05, 9.99819100e-01, 9.99819100e-01,\n",
       "       9.99819100e-01, 7.38742601e-05, 7.38742601e-05, 9.99819100e-01,\n",
       "       9.99819100e-01, 7.38742601e-05, 9.99819100e-01, 9.99819100e-01,\n",
       "       7.38742601e-05, 9.99819100e-01, 9.99819100e-01, 9.99819100e-01,\n",
       "       9.99819100e-01, 7.38742601e-05, 7.38742601e-05, 7.38742601e-05,\n",
       "       9.99819100e-01, 7.38742601e-05, 9.99819100e-01, 9.99819100e-01,\n",
       "       7.38742601e-05, 9.99819100e-01, 7.38742601e-05, 7.38742601e-05,\n",
       "       7.38742601e-05, 7.38742601e-05, 9.99819100e-01, 7.38742601e-05,\n",
       "       4.02976424e-01, 5.92767060e-01, 4.02976424e-01, 4.02976424e-01,\n",
       "       4.02976424e-01, 5.92767060e-01, 5.92767060e-01, 5.92767060e-01,\n",
       "       4.02976424e-01, 5.92767060e-01, 5.92767060e-01, 4.02976424e-01,\n",
       "       4.02976424e-01, 5.92767060e-01, 4.02976424e-01, 5.92767060e-01,\n",
       "       5.92767060e-01, 5.92767060e-01, 4.02976424e-01, 5.92767060e-01,\n",
       "       5.92767060e-01, 4.02976424e-01, 5.92767060e-01, 4.02976424e-01,\n",
       "       4.02976424e-01, 5.92767060e-01, 4.02976424e-01, 5.92767060e-01,\n",
       "       5.92767060e-01, 5.92767060e-01, 5.92767060e-01, 5.92767060e-01,\n",
       "       5.92767060e-01, 5.92767060e-01, 5.92767060e-01, 5.92767060e-01,\n",
       "       5.92767060e-01, 4.02976424e-01, 4.02976424e-01, 5.92767060e-01,\n",
       "       4.02976424e-01, 5.92767060e-01, 4.02976424e-01, 4.02976424e-01,\n",
       "       4.02976424e-01, 4.02976424e-01, 4.02976424e-01, 4.02976424e-01,\n",
       "       4.02976424e-01, 5.92767060e-01, 1.08179627e-02, 9.87764657e-01,\n",
       "       9.87764657e-01, 9.87764657e-01, 1.08179627e-02, 9.87764657e-01,\n",
       "       9.87764657e-01, 1.08179627e-02, 1.08179627e-02, 9.87764657e-01,\n",
       "       1.08179627e-02, 1.08179627e-02, 9.87764657e-01, 9.87764657e-01,\n",
       "       1.08179627e-02, 1.08179627e-02, 1.08179627e-02, 1.08179627e-02,\n",
       "       1.08179627e-02, 1.08179627e-02, 9.87764657e-01, 1.08179627e-02,\n",
       "       1.08179627e-02, 9.87764657e-01, 1.08179627e-02, 9.87764657e-01,\n",
       "       1.08179627e-02, 1.08179627e-02, 9.87764657e-01, 9.87764657e-01,\n",
       "       9.87764657e-01, 9.87764657e-01, 1.08179627e-02, 9.87764657e-01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_on_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
